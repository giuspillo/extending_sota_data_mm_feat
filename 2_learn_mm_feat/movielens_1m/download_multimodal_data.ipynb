{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "First, we need to install the yt_dlp package, the SPARQLWrapper, and the BeautifulSoup package, as these are the two packages used to download the multimodal features."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: yt_dlp in /Users/giuse/opt/miniconda3/lib/python3.9/site-packages (2025.3.26)\n",
      "Requirement already satisfied: SPARQLWrapper in /Users/giuse/opt/miniconda3/lib/python3.9/site-packages (2.0.0)\n",
      "Requirement already satisfied: rdflib>=6.1.1 in /Users/giuse/opt/miniconda3/lib/python3.9/site-packages (from SPARQLWrapper) (7.1.3)\n",
      "Requirement already satisfied: isodate<1.0.0,>=0.7.2 in /Users/giuse/opt/miniconda3/lib/python3.9/site-packages (from rdflib>=6.1.1->SPARQLWrapper) (0.7.2)\n",
      "Requirement already satisfied: pyparsing<4,>=2.1.0 in /Users/giuse/opt/miniconda3/lib/python3.9/site-packages (from rdflib>=6.1.1->SPARQLWrapper) (3.0.9)\n",
      "Requirement already satisfied: bs4 in /Users/giuse/opt/miniconda3/lib/python3.9/site-packages (0.0.2)\n",
      "Requirement already satisfied: beautifulsoup4 in /Users/giuse/opt/miniconda3/lib/python3.9/site-packages (from bs4) (4.11.1)\n",
      "Requirement already satisfied: soupsieve>1.2 in /Users/giuse/opt/miniconda3/lib/python3.9/site-packages (from beautifulsoup4->bs4) (2.3.2.post1)\n"
     ]
    }
   ],
   "source": [
    "!pip install yt_dlp\n",
    "!pip install SPARQLWrapper\n",
    "!pip install bs4"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "import yt_dlp\n",
    "from bs4 import BeautifulSoup\n",
    "import pandas as pd\n",
    "import requests\n",
    "from SPARQLWrapper import SPARQLWrapper, JSON"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To download the multimodal data, first we need to read the .tsv file that has the links of such data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "extended_mapping = pd.read_csv('ml1m_full_extended_mapping.tsv', sep='\\t')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>movie_id</th>\n",
       "      <th>dburl</th>\n",
       "      <th>wiki_url</th>\n",
       "      <th>img_url</th>\n",
       "      <th>name</th>\n",
       "      <th>youtubeId</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>2</td>\n",
       "      <td>http://dbpedia.org/resource/Jumanji</td>\n",
       "      <td>https://en.wikipedia.org/wiki/Jumanji</td>\n",
       "      <td>https://upload.wikimedia.org/wikipedia/en/b/b6...</td>\n",
       "      <td>Jumanji (1995)</td>\n",
       "      <td>3LPANjHlPxo</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>3</td>\n",
       "      <td>http://dbpedia.org/resource/Grumpier_Old_Men</td>\n",
       "      <td>https://en.wikipedia.org/wiki/Grumpier_Old_Men</td>\n",
       "      <td>https://upload.wikimedia.org/wikipedia/en/0/03...</td>\n",
       "      <td>Grumpier Old Men (1995)</td>\n",
       "      <td>rEnOoWs3FuA</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>4</td>\n",
       "      <td>http://dbpedia.org/resource/Waiting_to_Exhale</td>\n",
       "      <td>https://en.wikipedia.org/wiki/Waiting_to_Exhale</td>\n",
       "      <td>https://upload.wikimedia.org/wikipedia/en/c/ca...</td>\n",
       "      <td>Waiting to Exhale (1995)</td>\n",
       "      <td>j9xml1CxgXI</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>5</td>\n",
       "      <td>http://dbpedia.org/resource/Father_of_the_Brid...</td>\n",
       "      <td>https://en.wikipedia.org/wiki/Father_of_the_Br...</td>\n",
       "      <td>https://upload.wikimedia.org/wikipedia/en/e/e1...</td>\n",
       "      <td>Father of the Bride Part II (1995)</td>\n",
       "      <td>BbvnDlu_Zjc</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>6</td>\n",
       "      <td>http://dbpedia.org/resource/Heat_(1995_film)</td>\n",
       "      <td>https://en.wikipedia.org/wiki/Heat_(1995_film)</td>\n",
       "      <td>https://upload.wikimedia.org/wikipedia/en/6/6c...</td>\n",
       "      <td>Heat (1995)</td>\n",
       "      <td>2GfZl4kuVNI</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   movie_id                                              dburl  \\\n",
       "0         2                http://dbpedia.org/resource/Jumanji   \n",
       "1         3       http://dbpedia.org/resource/Grumpier_Old_Men   \n",
       "2         4      http://dbpedia.org/resource/Waiting_to_Exhale   \n",
       "3         5  http://dbpedia.org/resource/Father_of_the_Brid...   \n",
       "4         6       http://dbpedia.org/resource/Heat_(1995_film)   \n",
       "\n",
       "                                            wiki_url  \\\n",
       "0              https://en.wikipedia.org/wiki/Jumanji   \n",
       "1     https://en.wikipedia.org/wiki/Grumpier_Old_Men   \n",
       "2    https://en.wikipedia.org/wiki/Waiting_to_Exhale   \n",
       "3  https://en.wikipedia.org/wiki/Father_of_the_Br...   \n",
       "4     https://en.wikipedia.org/wiki/Heat_(1995_film)   \n",
       "\n",
       "                                             img_url  \\\n",
       "0  https://upload.wikimedia.org/wikipedia/en/b/b6...   \n",
       "1  https://upload.wikimedia.org/wikipedia/en/0/03...   \n",
       "2  https://upload.wikimedia.org/wikipedia/en/c/ca...   \n",
       "3  https://upload.wikimedia.org/wikipedia/en/e/e1...   \n",
       "4  https://upload.wikimedia.org/wikipedia/en/6/6c...   \n",
       "\n",
       "                                 name    youtubeId  \n",
       "0                      Jumanji (1995)  3LPANjHlPxo  \n",
       "1             Grumpier Old Men (1995)  rEnOoWs3FuA  \n",
       "2            Waiting to Exhale (1995)  j9xml1CxgXI  \n",
       "3  Father of the Bride Part II (1995)  BbvnDlu_Zjc  \n",
       "4                         Heat (1995)  2GfZl4kuVNI  "
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "extended_mapping.head(5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This function saves a YT video in .mp4 format, given its link, into the correct folder."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def download_youtube_video(youtube_id, item_id):\n",
    "    url = f'https://www.youtube.com/watch?v={youtube_id}'\n",
    "    filename = f'_videos/{item_id}.mp4'\n",
    "    ydl_opts = {\n",
    "        'outtmpl': filename,\n",
    "        'format': 'bestvideo+bestaudio/best'\n",
    "    }\n",
    "    with yt_dlp.YoutubeDL(ydl_opts) as ydl:\n",
    "        ydl.download([url])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Similarly, these functions are used to scrape the wikipedia page associated to a movie (through the DBpedia uri), in order to get the movie poster; then, starting from the DBpedia uri, we also get the plot of the movie."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "def convert_url(dbpedia_url):\n",
    "    dbpart = dbpedia_url.split('/')[-1]\n",
    "    return 'https://en.wikipedia.org/wiki/' + dbpart\n",
    "\n",
    "def get_movie_poster(dbpedia_url, item_id):\n",
    "    wikipedia_url = convert_url(dbpedia_url)\n",
    "    response = requests.get(wikipedia_url)\n",
    "    soup = BeautifulSoup(response.text, 'html.parser')\n",
    "    infobox = soup.find('table', {'class': 'infobox'})\n",
    "    if infobox:\n",
    "        img_tag = infobox.find('img')\n",
    "        if img_tag:\n",
    "            img_url = 'https:' + img_tag['src']\n",
    "            img_data = requests.get(img_url).content\n",
    "            with open(f'_posters/{item_id}.jpg', 'wb') as handler:\n",
    "                handler.write(img_data)\n",
    "            return img_url\n",
    "\n",
    "def get_movie_plot(dbpedia_uri):\n",
    "    sparql = SPARQLWrapper(\"https://dbpedia.org/sparql\")\n",
    "    query = f\"\"\"\n",
    "    SELECT ?abstract WHERE {{\n",
    "        <{dbpedia_uri}> dbo:abstract ?abstract .\n",
    "        FILTER (lang(?abstract) = \"en\")\n",
    "    }}\n",
    "    \"\"\"\n",
    "    sparql.setQuery(query)\n",
    "    sparql.setReturnFormat(JSON)\n",
    "    try:\n",
    "        results = sparql.query().convert()\n",
    "        for result in results[\"results\"][\"bindings\"]:\n",
    "            return result[\"abstract\"][\"value\"]\n",
    "        return \"No abstract found.\"\n",
    "    except Exception as e:\n",
    "        return f\"Error: {e}\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The original dataframe is iterated to gather all multimodal data we are interest in."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "http://dbpedia.org/resource/Jumanji\n",
      "[youtube] Extracting URL: https://www.youtube.com/watch?v=3LPANjHlPxo\n",
      "[youtube] 3LPANjHlPxo: Downloading webpage\n",
      "[youtube] 3LPANjHlPxo: Downloading tv client config\n",
      "[youtube] 3LPANjHlPxo: Downloading player 4fcd6e4a\n",
      "[youtube] 3LPANjHlPxo: Downloading tv player API JSON\n",
      "[youtube] 3LPANjHlPxo: Downloading ios player API JSON\n",
      "[youtube] 3LPANjHlPxo: Downloading m3u8 information\n",
      "[info] 3LPANjHlPxo: Downloading 1 format(s): 136+251\n",
      "[download] _videos/2.mp4.mkv has already been downloaded\n"
     ]
    }
   ],
   "source": [
    "for i, row in extended_mapping.iterrows():\n",
    "\n",
    "    item_id = row['movie_id']\n",
    "    dburl = row['dburl']\n",
    "    yt_id = row['youtubeId']\n",
    "\n",
    "    print(dburl)\n",
    "\n",
    "    # this downloads the movie plot\n",
    "    plot = get_movie_plot(dburl)\n",
    "\n",
    "    # this stores the movie poster in the _posters folder\n",
    "    get_movie_poster(dburl, item_id)\n",
    "\n",
    "    # this stores the movie trailer in the _videos folder\n",
    "    download_youtube_video(yt_id, item_id)\n",
    "\n",
    "\n",
    "    break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
