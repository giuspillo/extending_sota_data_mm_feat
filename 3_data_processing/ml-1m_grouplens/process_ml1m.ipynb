{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# repro processing ml1m"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "   user  item  rating  timestamp\n",
       "0     1  1193       5  978300760\n",
       "1     1   661       3  978302109\n",
       "2     1   914       3  978301968\n",
       "3     1  3408       4  978300275\n",
       "4     1  2355       5  978824291\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# read ML1M ratings provided by grouplens: https://grouplens.org/datasets/movielens/ \n",
    "ratings = pd.read_csv('original_data/ratings.dat', sep='::', names=['user', 'item', 'rating', 'timestamp'])\n",
    "ratings.head(5)\n",
    "print(ratings.head(5))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "   id                                name                        genres\n",
       "0   1                    Toy Story (1995)   Animation|Children's|Comedy\n",
       "1   2                      Jumanji (1995)  Adventure|Children's|Fantasy\n",
       "2   3             Grumpier Old Men (1995)                Comedy|Romance\n",
       "3   4            Waiting to Exhale (1995)                  Comedy|Drama\n",
       "4   5  Father of the Bride Part II (1995)                        Comedy\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# read movie metadata\n",
    "movies = pd.read_csv('original_data/movies.dat', sep='::', names=['id', 'name', 'genres'], encoding='ISO-8859-1')\n",
    "print(movies.head(5))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Ratings: 1000209\n",
       "Users: 6040\n",
       "Items: 3706\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# print dataset statistics\n",
    "print(f'Ratings: {len(ratings)}')\n",
    "print(f'Users: {len(ratings[\"user\"].unique())}')\n",
    "print(f'Items: {len(ratings[\"item\"].unique())}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# apply core-5 filtering to interactions\n",
    "\n",
    "def core_k_filtering(interactions, k):\n",
    "    \"\"\"\n",
    "    Perform Core5 filtering on a user-item-rating DataFrame.\n",
    "    Ensures that every user and item has at least 5 interactions.\n",
    "    \"\"\"\n",
    "    while True:\n",
    "        user_counts = interactions['user'].value_counts()\n",
    "        item_counts = interactions['item'].value_counts()\n",
    "        \n",
    "        valid_users = user_counts[user_counts >= 5].index\n",
    "        valid_items = item_counts[item_counts >= 5].index\n",
    "        \n",
    "        core_k = interactions[interactions['user'].isin(valid_users) & interactions['item'].isin(valid_items)]\n",
    "        \n",
    "        if len(core_k) == len(interactions):\n",
    "            break\n",
    "        \n",
    "        interactions = core_k\n",
    "\n",
    "    return core_k\n",
    "\n",
    "ratings_core5 = core_k_filtering(ratings, 5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Ratings: 999611\n",
       "Users: 6040\n",
       "Items: 3416\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# print stats of the core-5 dataset\n",
    "print(f'Ratings: {len(ratings_core5)}')\n",
    "print(f'Users: {len(ratings_core5[\"user\"].unique())}')\n",
    "print(f'Items: {len(ratings_core5[\"item\"].unique())}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "   movie_id  ...    youtubeId\n",
       "0         2  ...  3LPANjHlPxo\n",
       "1         3  ...  rEnOoWs3FuA\n",
       "2         4  ...  j9xml1CxgXI\n",
       "3         5  ...  BbvnDlu_Zjc\n",
       "4         6  ...  2GfZl4kuVNI\n",
       "\n",
       "[5 rows x 6 columns]\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# read our extended mapping\n",
    "multimodal_mapping = pd.read_csv('ml1m_full_extended_mapping.tsv', sep='\\t')\n",
    "print(multimodal_mapping.head(5))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Out extended mapping has 3197 items\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "mapped_items = set(multimodal_mapping['movie_id'])\n",
    "print(f'Out extended mapping has {len(mapped_items)} items')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "In total, there are 365 items for which we do not have the movie poster, the movie trailer, or the movie plot\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "missing = len(set(ratings_core5['item']) - mapped_items)\n",
    "print(f'In total, there are {missing} items for which we do not have the movie poster, the movie trailer, or the movie plot')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Ratings: 946780\n",
       "Users: 6040\n",
       "Items: 3051\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# filter out the rows involving the missing items\n",
    "multimodal_ratings = ratings_core5[ratings_core5['item'].isin(mapped_items)]\n",
    "\n",
    "# print the updated statistics\n",
    "print(f'Ratings: {len(multimodal_ratings)}')\n",
    "print(f'Users: {len(multimodal_ratings[\"user\"].unique())}')\n",
    "print(f'Items: {len(multimodal_ratings[\"item\"].unique())}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Now we need convert each .pkl file learned during the extraction process into an .npy file, stored as np.array from 0 to k-1, where k is the total number of items\n",
    "# Moreover, as required by MMRec, we need to provide .npy files with the same number of items\n",
    "\n",
    "# For this reason, we need to filter out all the items that have no any of the multimodal features. \n",
    "# Note that this has been done only in our setting to perform the experiments, but in your case few modalities might be enough,\n",
    "# so you might need to filter out less items or no items at all.\n",
    "\n",
    "# Moreover, this is needed for the MMRec framework, but if you are using other framework - or you own one - this might be not necessary.\n",
    "# This is also the reason we choose to provide the .pkl dict files, in such a way any user can use them the way they need and want to."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "whisper.pkl\n",
       "all-mpnet-base-v2.pkl\n",
       "r2p1d.pkl\n",
       "vit_cls.pkl\n",
       "all-MiniLM-L6-v2.pkl\n",
       "vggish.pkl\n",
       "resnet152.pkl\n",
       "vit_avg.pkl\n",
       "vgg.pkl\n",
       "3096\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Read all the items for which we have all the multimodal features.\n",
    "# This is required by MMRec, but is not mandatory if you focus only on a sinlge modality or a subset of them\n",
    "\n",
    "import pickle as pkl\n",
    "import os \n",
    "import numpy as np\n",
    "\n",
    "item_keys = None\n",
    "\n",
    "for pkl_file in os.listdir('multimodal_features/dict/'):\n",
    "\n",
    "    if '.pkl' in pkl_file:\n",
    "        print(pkl_file)\n",
    "        data_dict = pkl.load(open(f'multimodal_features/dict/{pkl_file}', 'rb'))\n",
    "\n",
    "        if item_keys is None:\n",
    "            item_keys = set(data_dict.keys())\n",
    "        else:\n",
    "            item_keys &= set(data_dict.keys())\n",
    "print(len(item_keys))\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "3096\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "multimodal_ratings = multimodal_ratings[multimodal_ratings['item'].isin(item_keys)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Ratings: 942799\n",
       "Users: 6040\n",
       "Items: 2981\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# print the updated statistics\n",
    "print(f'Ratings: {len(multimodal_ratings)}')\n",
    "print(f'Users: {len(multimodal_ratings[\"user\"].unique())}')\n",
    "print(f'Items: {len(multimodal_ratings[\"item\"].unique())}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Len training: 751879\n",
       "Len validation: 91603\n",
       "Len testing: 99317\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# split into train, valid, test - in this case, we use the same format required by MMRec, but any strategy can be applied\n",
    "# to avoid any error in the training of the models, we split into 80-10-10 and ensure that all users and items appear in the training\n",
    "\n",
    "import numpy as np\n",
    "\n",
    "def split_data(df, train_ratio=0.8, valid_ratio=0.1, test_ratio=0.1, seed=42):\n",
    "    \"\"\"\n",
    "    Splits a dataset into train, validation, and test sets while ensuring that:\n",
    "    - All users and items appear in the training set.\n",
    "    - Validation and test sets do not introduce new users or items.\n",
    "    \"\"\"\n",
    "    np.random.seed(seed)\n",
    "    \n",
    "    train_list, valid_list, test_list = [], [], []\n",
    "    \n",
    "    for _, user_df in df.groupby(\"user\"):\n",
    "        user_df = user_df.sample(frac=1, random_state=seed)\n",
    "        \n",
    "        num_interactions = len(user_df)\n",
    "        train_end = int(num_interactions * train_ratio)\n",
    "        valid_end = train_end + int(num_interactions * valid_ratio)\n",
    "        \n",
    "        train_list.append(user_df.iloc[:train_end])\n",
    "        valid_list.append(user_df.iloc[train_end:valid_end])\n",
    "        test_list.append(user_df.iloc[valid_end:])\n",
    "    \n",
    "    train_df = pd.concat(train_list).reset_index(drop=True)\n",
    "    valid_df = pd.concat(valid_list).reset_index(drop=True)\n",
    "    test_df = pd.concat(test_list).reset_index(drop=True)\n",
    "    \n",
    "    # Ensure valid & test sets contain only users & items from training\n",
    "    train_users, train_items = set(train_df['user']), set(train_df['item'])\n",
    "    \n",
    "    valid_df = valid_df[valid_df['user'].isin(train_users) & valid_df['item'].isin(train_items)].reset_index(drop=True)\n",
    "    test_df = test_df[test_df['user'].isin(train_users) & test_df['item'].isin(train_items)].reset_index(drop=True)\n",
    "    \n",
    "    return train_df, valid_df, test_df\n",
    "\n",
    "# split data into train, valid, test\n",
    "train, valid, test = split_data(multimodal_ratings)\n",
    "\n",
    "print(f'Len training: {len(train)}')\n",
    "print(f'Len validation: {len(valid)}')\n",
    "print(f'Len testing: {len(test)}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# format data into MMRec format\n",
    "# MMRec requires a single .inter file, with a column named 'x_label' that can assume 3 possible values:\n",
    "# - 0: the interaction is in the training set\n",
    "# - 1: the interaction is in the validation set\n",
    "# - 2: the interaction is in the test set\n",
    "train['x_label'] = 0\n",
    "valid['x_label'] = 1\n",
    "test['x_label'] = 2\n",
    "\n",
    "# now, we concat the three dataframe to save the unique .inter file required by MMRec\n",
    "split_data = pd.concat([train, valid, test])\n",
    "\n",
    "# binarize rating\n",
    "# if rating <= 3    --> 0\n",
    "# if rating > 4     --> 1\n",
    "split_data['rating'] = split_data['rating'].apply(lambda x: 0 if x <= 3 else 1)\n",
    "\n",
    "# rename columns\n",
    "split_data.columns = ['userID', 'itemID', 'rating', 'timestamp', 'x_label']\n",
    "\n",
    "split_data.to_csv('movielens_1m.inter', sep='\\t', index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# build the maps for users and item, so that their IDs go from 0 to x-1\n",
    "\n",
    "map_user = {userID: new_id for new_id, userID in enumerate(split_data['userID'].unique())}\n",
    "map_item = {itemID: new_id for new_id, itemID in enumerate(split_data['itemID'].unique())}\n",
    "\n",
    "# inverse_map_user = {new_id: userID for new_id, userID in map_user.items()}\n",
    "inverse_map_item = {new_id: itemID for itemID, new_id in map_item.items()}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "whisper\n",
       "all-mpnet-base-v2\n",
       "r2p1d\n",
       "vit_cls\n",
       "all-MiniLM-L6-v2\n",
       "vggish\n",
       "resnet152\n",
       "vit_avg\n",
       "vgg\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# for each dict of embedding:\n",
    "# 1. get the items that must be kept\n",
    "# 2. map with the new IDs\n",
    "# 3. store them as np.array\n",
    "\n",
    "for pkl_file in os.listdir('multimodal_features/dict/'):\n",
    "\n",
    "    if '.pkl' in pkl_file:\n",
    "\n",
    "        name_mod = pkl_file.split('.')[0]\n",
    "        print(name_mod)\n",
    "\n",
    "        emb_list = []\n",
    "        \n",
    "        data_dict = pkl.load(open(f'multimodal_features/dict/{pkl_file}', 'rb'))\n",
    "\n",
    "        for index in range(len(inverse_map_item)):\n",
    "            old_id = inverse_map_item[index]\n",
    "            emb = data_dict[old_id]\n",
    "            emb_list.append(emb)\n",
    "        \n",
    "        emb_array = np.array(emb_list)\n",
    "        np.save(open(f'multimodal_features/mmrec_npy/{name_mod}.npy', 'wb'), emb_array)\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Now we remap the original dataset and save the remapped version\n",
    "split_data['userID'] = split_data['userID'].map(map_user)\n",
    "split_data['itemID'] = split_data['itemID'].map(map_item)\n",
    "\n",
    "split_data.to_csv('processed_data/movielens_1m.inter', sep='\\t', index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# just to be sure that numpy library version mismatch should affect the readability of our embeddings, \n",
    "# we save the multimodal features as json files as well\n",
    "\n",
    "import pickle as pkl\n",
    "import os \n",
    "import numpy as np\n",
    "import json\n",
    "\n",
    "item_keys = None\n",
    "\n",
    "for pkl_file in os.listdir('multimodal_features/dict/'):\n",
    "\n",
    "    if '.pkl' in pkl_file:\n",
    "        print(pkl_file)\n",
    "        data_dict = pkl.load(open(f'multimodal_features/dict/{pkl_file}', 'rb'))\n",
    "        \n",
    "        data_dict_serializable = {key: value.tolist() for key, value in data_dict.items()}\n",
    "        with open(f'multimodal_features/json/{pkl_file.split(\".pkl\")[0]}.json', 'w') as f:\n",
    "            json.dump(data_dict_serializable, f, indent=4)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
