{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# repro processing dbbook"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# read train and test split provided by https://github.com/swapUniba/Deep_CBRS_Amar/tree/master/datasets/dbbook\n",
    "train = pd.read_csv('original_data/train.tsv', sep='\\t', names=['userID', 'itemID', 'rating'])\n",
    "test = pd.read_csv('original_data/test.tsv', sep='\\t', names=['userID', 'itemID', 'rating'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>userID</th>\n",
       "      <th>itemID</th>\n",
       "      <th>rating</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>6873</td>\n",
       "      <td>5950</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>6873</td>\n",
       "      <td>8010</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>6873</td>\n",
       "      <td>5232</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>6873</td>\n",
       "      <td>7538</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>6873</td>\n",
       "      <td>5231</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   userID  itemID  rating\n",
       "0    6873    5950       1\n",
       "1    6873    8010       1\n",
       "2    6873    5232       1\n",
       "3    6873    7538       1\n",
       "4    6873    5231       0"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "ratings = pd.concat([train, test])\n",
    "ratings.head(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Ratings: 140360\n",
      "Users: 6181\n",
      "Items: 7672\n"
     ]
    }
   ],
   "source": [
    "#Â print dataset statistics\n",
    "print(f'Ratings: {len(ratings)}')\n",
    "print(f'Users: {len(ratings[\"userID\"].unique())}')\n",
    "print(f'Items: {len(ratings[\"itemID\"].unique())}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# apply core-5 filtering to interactions\n",
    "\n",
    "def core_k_filtering(interactions, k):\n",
    "    \"\"\"\n",
    "    Perform Core5 filtering on a user-item-rating DataFrame.\n",
    "    Ensures that every user and item has at least 5 interactions.\n",
    "    \"\"\"\n",
    "    while True:\n",
    "        user_counts = interactions['userID'].value_counts()\n",
    "        item_counts = interactions['itemID'].value_counts()\n",
    "        \n",
    "        valid_users = user_counts[user_counts >= 5].index\n",
    "        valid_items = item_counts[item_counts >= 5].index\n",
    "        \n",
    "        core_k = interactions[interactions['userID'].isin(valid_users) & interactions['itemID'].isin(valid_items)]\n",
    "        \n",
    "        if len(core_k) == len(interactions):\n",
    "            break\n",
    "        \n",
    "        interactions = core_k\n",
    "\n",
    "    return core_k\n",
    "\n",
    "ratings_core5 = core_k_filtering(ratings, 5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Ratings: 132837\n",
      "Users: 6179\n",
      "Items: 4622\n"
     ]
    }
   ],
   "source": [
    "# print stats of the core-5 dataset\n",
    "print(f'Ratings: {len(ratings_core5)}')\n",
    "print(f'Users: {len(ratings_core5[\"userID\"].unique())}')\n",
    "print(f'Items: {len(ratings_core5[\"itemID\"].unique())}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "6170\n",
      "4197\n"
     ]
    }
   ],
   "source": [
    "# load extended mapping and texts\n",
    "extended_mapping = pd.read_csv('full_extended_dbbook_img_links.tsv', sep='\\t')\n",
    "texts = pd.read_csv('dbbook_texts.tsv', sep='\\t')\n",
    "\n",
    "# load item IDs\n",
    "mapping_ids = set(extended_mapping['id'])\n",
    "texts = set(texts['id'])\n",
    "\n",
    "core5_ids = set(ratings_core5['itemID'])\n",
    "\n",
    "print(len(mapping_ids.intersection(texts)))\n",
    "print(len(mapping_ids.intersection(texts).intersection(core5_ids)))\n",
    "\n",
    "keep_items = mapping_ids.intersection(texts).intersection(core5_ids)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Ratings: 121924\n",
      "Users: 6179\n",
      "Items: 4197\n"
     ]
    }
   ],
   "source": [
    "multimodal_ratings = ratings_core5[ratings_core5['itemID'].isin(keep_items)]\n",
    "# print stats of the core-5 dataset\n",
    "print(f'Ratings: {len(multimodal_ratings)}')\n",
    "print(f'Users: {len(multimodal_ratings[\"userID\"].unique())}')\n",
    "print(f'Items: {len(multimodal_ratings[\"itemID\"].unique())}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Len training: 95075\n",
      "Len valid: 9489\n",
      "Len test: 17350\n"
     ]
    }
   ],
   "source": [
    "# split into train, valid, test - in this case, we use the same format required by MMRec, but any strategy can be applied\n",
    "# to avoid any error in the training of the models, we split into 80-10-10 and ensure that all users and items appear in the training\n",
    "\n",
    "import numpy as np\n",
    "\n",
    "def split_data(df, train_ratio=0.8, valid_ratio=0.1, test_ratio=0.1, seed=42):\n",
    "    \"\"\"\n",
    "    Splits a dataset into train, validation, and test sets while ensuring that:\n",
    "    - All users and items appear in the training set.\n",
    "    - Validation and test sets do not introduce new users or items.\n",
    "    \"\"\"\n",
    "    np.random.seed(seed)\n",
    "    \n",
    "    train_list, valid_list, test_list = [], [], []\n",
    "    \n",
    "    for _, user_df in df.groupby(\"userID\"):\n",
    "        user_df = user_df.sample(frac=1, random_state=seed)\n",
    "        \n",
    "        num_interactions = len(user_df)\n",
    "        train_end = int(num_interactions * train_ratio)\n",
    "        valid_end = train_end + int(num_interactions * valid_ratio)\n",
    "        \n",
    "        train_list.append(user_df.iloc[:train_end])\n",
    "        valid_list.append(user_df.iloc[train_end:valid_end])\n",
    "        test_list.append(user_df.iloc[valid_end:])\n",
    "    \n",
    "    train_df = pd.concat(train_list).reset_index(drop=True)\n",
    "    valid_df = pd.concat(valid_list).reset_index(drop=True)\n",
    "    test_df = pd.concat(test_list).reset_index(drop=True)\n",
    "    \n",
    "    # Ensure valid & test sets contain only users & items from training\n",
    "    train_users, train_items = set(train_df['userID']), set(train_df['itemID'])\n",
    "    \n",
    "    valid_df = valid_df[valid_df['userID'].isin(train_users) & valid_df['itemID'].isin(train_items)].reset_index(drop=True)\n",
    "    test_df = test_df[test_df['userID'].isin(train_users) & test_df['itemID'].isin(train_items)].reset_index(drop=True)\n",
    "    \n",
    "    return train_df, valid_df, test_df\n",
    "\n",
    "# split data into train, valid, test\n",
    "train, valid, test = split_data(multimodal_ratings)\n",
    "\n",
    "print(f'Len training: {len(train)}')\n",
    "print(f'Len valid: {len(valid)}')\n",
    "print(f'Len test: {len(test)}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "# format data into MMRec format\n",
    "# MMRec requires a single .inter file, with a column named 'x_label' that can assume 3 possible values:\n",
    "# - 0: the interaction is in the training set\n",
    "# - 1: the interaction is in the validation set\n",
    "# - 2: the interaction is in the test set\n",
    "train['x_label'] = 0\n",
    "valid['x_label'] = 1\n",
    "test['x_label'] = 2\n",
    "\n",
    "# now, we concat the three dataframe to save the unique .inter file required by MMRec\n",
    "split_data = pd.concat([train, valid, test])\n",
    "\n",
    "# rename columns\n",
    "split_data.columns = ['userID', 'itemID', 'rating', 'x_label']\n",
    "\n",
    "# we save this dataset with the original IDs\n",
    "split_data.to_csv('processed_data/dbbook_og_ids.inter', sep='\\t', index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "# we need to remap both user and item IDs from 0 to n-1\n",
    "\n",
    "map_users = {user_id: i for i, user_id in enumerate(split_data['userID'].unique())}\n",
    "map_items = {item_id: i for i, item_id in enumerate(split_data['itemID'].unique())}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "split_data['userID'] = split_data['userID'].map(map_users)\n",
    "split_data['itemID'] = split_data['itemID'].map(map_items)\n",
    "split_data.to_csv('processed_data/dbbook.inter', sep='\\t', index=False)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "# finally, use these mapping to remap the multimodal features and save them a np.array"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
