{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# repro processing ml1m"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_1268655/2700901930.py:2: ParserWarning: Falling back to the 'python' engine because the 'c' engine does not support regex separators (separators > 1 char and different from '\\s+' are interpreted as regex); you can avoid this warning by specifying engine='python'.\n",
      "  ratings = pd.read_csv('original_data/ratings.dat', sep='::', names=['user', 'item', 'rating', 'timestamp'])\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "   user  item  rating  timestamp\n",
      "0     1  1193       5  978300760\n",
      "1     1   661       3  978302109\n",
      "2     1   914       3  978301968\n",
      "3     1  3408       4  978300275\n",
      "4     1  2355       5  978824291\n"
     ]
    }
   ],
   "source": [
    "# read ML1M ratings provided by grouplens: https://grouplens.org/datasets/movielens/ \n",
    "ratings = pd.read_csv('original_data/ratings.dat', sep='::', names=['user', 'item', 'rating', 'timestamp'])\n",
    "ratings.head(5)\n",
    "print(ratings.head(5))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "   id                                name                        genres\n",
      "0   1                    Toy Story (1995)   Animation|Children's|Comedy\n",
      "1   2                      Jumanji (1995)  Adventure|Children's|Fantasy\n",
      "2   3             Grumpier Old Men (1995)                Comedy|Romance\n",
      "3   4            Waiting to Exhale (1995)                  Comedy|Drama\n",
      "4   5  Father of the Bride Part II (1995)                        Comedy\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_1268655/1794383094.py:2: ParserWarning: Falling back to the 'python' engine because the 'c' engine does not support regex separators (separators > 1 char and different from '\\s+' are interpreted as regex); you can avoid this warning by specifying engine='python'.\n",
      "  movies = pd.read_csv('original_data/movies.dat', sep='::', names=['id', 'name', 'genres'], encoding='ISO-8859-1')\n"
     ]
    }
   ],
   "source": [
    "# read movie metadata\n",
    "movies = pd.read_csv('original_data/movies.dat', sep='::', names=['id', 'name', 'genres'], encoding='ISO-8859-1')\n",
    "print(movies.head(5))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Ratings: 1000209\n",
      "Users: 6040\n",
      "Items: 3706\n"
     ]
    }
   ],
   "source": [
    "#Â print dataset statistics\n",
    "print(f'Ratings: {len(ratings)}')\n",
    "print(f'Users: {len(ratings[\"user\"].unique())}')\n",
    "print(f'Items: {len(ratings[\"item\"].unique())}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# apply core-5 filtering to interactions\n",
    "\n",
    "def core_k_filtering(interactions, k):\n",
    "    \"\"\"\n",
    "    Perform Core5 filtering on a user-item-rating DataFrame.\n",
    "    Ensures that every user and item has at least 5 interactions.\n",
    "    \"\"\"\n",
    "    while True:\n",
    "        user_counts = interactions['user'].value_counts()\n",
    "        item_counts = interactions['item'].value_counts()\n",
    "        \n",
    "        valid_users = user_counts[user_counts >= 5].index\n",
    "        valid_items = item_counts[item_counts >= 5].index\n",
    "        \n",
    "        core_k = interactions[interactions['user'].isin(valid_users) & interactions['item'].isin(valid_items)]\n",
    "        \n",
    "        if len(core_k) == len(interactions):\n",
    "            break\n",
    "        \n",
    "        interactions = core_k\n",
    "\n",
    "    return core_k\n",
    "\n",
    "ratings_core5 = core_k_filtering(ratings, 5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Ratings: 999611\n",
      "Users: 6040\n",
      "Items: 3416\n"
     ]
    }
   ],
   "source": [
    "# print stats of the core-5 dataset\n",
    "print(f'Ratings: {len(ratings_core5)}')\n",
    "print(f'Users: {len(ratings_core5[\"user\"].unique())}')\n",
    "print(f'Items: {len(ratings_core5[\"item\"].unique())}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "   movie_id                                              dburl  \\\n",
      "0         2                http://dbpedia.org/resource/Jumanji   \n",
      "1         3       http://dbpedia.org/resource/Grumpier_Old_Men   \n",
      "2         4      http://dbpedia.org/resource/Waiting_to_Exhale   \n",
      "3         5  http://dbpedia.org/resource/Father_of_the_Brid...   \n",
      "4         6       http://dbpedia.org/resource/Heat_(1995_film)   \n",
      "\n",
      "                                            wiki_url  \\\n",
      "0              https://en.wikipedia.org/wiki/Jumanji   \n",
      "1     https://en.wikipedia.org/wiki/Grumpier_Old_Men   \n",
      "2    https://en.wikipedia.org/wiki/Waiting_to_Exhale   \n",
      "3  https://en.wikipedia.org/wiki/Father_of_the_Br...   \n",
      "4     https://en.wikipedia.org/wiki/Heat_(1995_film)   \n",
      "\n",
      "                                             img_url  \\\n",
      "0  https://upload.wikimedia.org/wikipedia/en/b/b6...   \n",
      "1  https://upload.wikimedia.org/wikipedia/en/0/03...   \n",
      "2  https://upload.wikimedia.org/wikipedia/en/c/ca...   \n",
      "3  https://upload.wikimedia.org/wikipedia/en/e/e1...   \n",
      "4  https://upload.wikimedia.org/wikipedia/en/6/6c...   \n",
      "\n",
      "                                 name    youtubeId  \n",
      "0                      Jumanji (1995)  3LPANjHlPxo  \n",
      "1             Grumpier Old Men (1995)  rEnOoWs3FuA  \n",
      "2            Waiting to Exhale (1995)  j9xml1CxgXI  \n",
      "3  Father of the Bride Part II (1995)  BbvnDlu_Zjc  \n",
      "4                         Heat (1995)  2GfZl4kuVNI  \n"
     ]
    }
   ],
   "source": [
    "# read our extended mapping\n",
    "multimodal_mapping = pd.read_csv('ml1m_full_extended_mapping.tsv', sep='\\t')\n",
    "print(multimodal_mapping.head(5))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Out extended mapping has 3197 items\n"
     ]
    }
   ],
   "source": [
    "mapped_items = set(multimodal_mapping['movie_id'])\n",
    "print(f'Out extended mapping has {len(mapped_items)} items')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "In total, there are 365 items for which we do not have the movie poster, the movie trailer, or the movie plot\n"
     ]
    }
   ],
   "source": [
    "missing = len(set(ratings_core5['item']) - mapped_items)\n",
    "print(f'In total, there are {missing} items for which we do not have the movie poster, the movie trailer, or the movie plot')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Ratings: 946780\n",
      "Users: 6040\n",
      "Items: 3051\n"
     ]
    }
   ],
   "source": [
    "# filter out the rows involving the missing items\n",
    "multimodal_ratings = ratings_core5[ratings_core5['item'].isin(mapped_items)]\n",
    "\n",
    "# print the updated statistics\n",
    "print(f'Ratings: {len(multimodal_ratings)}')\n",
    "print(f'Users: {len(multimodal_ratings[\"user\"].unique())}')\n",
    "print(f'Items: {len(multimodal_ratings[\"item\"].unique())}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Now we need convert each .pkl file learned during the extraction process into an .npy file, stored as np.array from 0 to k-1, where k is the total number of items\n",
    "# Moreover, as required by MMRec, we need to provide .npy files with the same number of items\n",
    "\n",
    "# For this reason, we need to filter out all the items that have no any of the multimodal features. \n",
    "# Note that this has been done only in our setting to perform the experiments, but in your case few modalities might be enough,\n",
    "# so you might need to filter out less items or no items at all.\n",
    "\n",
    "# Moreover, this is needed for the MMRec framework, but if you are using other framework - or you own one - this might be not necessary.\n",
    "# This is also the reason we choose to provide the .pkl dict files, in such a way any user can use them the way they need and want to."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "whisper.pkl\n",
      "all-mpnet-base-v2.pkl\n",
      "r2p1d.pkl\n",
      "vit_cls.pkl\n",
      "all-MiniLM-L6-v2.pkl\n",
      "vggish.pkl\n",
      "resnet152.pkl\n",
      "vit_avg.pkl\n",
      "i3d.pkl\n",
      "vgg.pkl\n",
      "3095\n"
     ]
    }
   ],
   "source": [
    "# Read all the items for which we have all the multimodal features.\n",
    "# This is required by MMRec, but is not mandatory if you focus only on a sinlge modality or a subset of them\n",
    "\n",
    "import pickle as pkl\n",
    "import os \n",
    "import numpy as np\n",
    "\n",
    "item_keys = None\n",
    "\n",
    "for pkl_file in os.listdir('multimodal_features/dict/'):\n",
    "\n",
    "    if '.pkl' in pkl_file:\n",
    "        print(pkl_file)\n",
    "        data_dict = pkl.load(open(f'multimodal_features/dict/{pkl_file}', 'rb'))\n",
    "\n",
    "        if item_keys is None:\n",
    "            item_keys = set(data_dict.keys())\n",
    "        else:\n",
    "            item_keys &= set(data_dict.keys())\n",
    "print(len(item_keys))\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "multimodal_ratings = multimodal_ratings[multimodal_ratings['item'].isin(item_keys)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Ratings: 942703\n",
      "Users: 6040\n",
      "Items: 2980\n"
     ]
    }
   ],
   "source": [
    "# print the updated statistics\n",
    "print(f'Ratings: {len(multimodal_ratings)}')\n",
    "print(f'Users: {len(multimodal_ratings[\"user\"].unique())}')\n",
    "print(f'Items: {len(multimodal_ratings[\"item\"].unique())}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Len training: 751795\n",
      "Len validation: 91586\n",
      "Len testing: 99322\n"
     ]
    }
   ],
   "source": [
    "# split into train, valid, test - in this case, we use the same format required by MMRec, but any strategy can be applied\n",
    "# to avoid any error in the training of the models, we split into 80-10-10 and ensure that all users and items appear in the training\n",
    "\n",
    "import numpy as np\n",
    "\n",
    "def split_data(df, train_ratio=0.8, valid_ratio=0.1, test_ratio=0.1, seed=42):\n",
    "    \"\"\"\n",
    "    Splits a dataset into train, validation, and test sets while ensuring that:\n",
    "    - All users and items appear in the training set.\n",
    "    - Validation and test sets do not introduce new users or items.\n",
    "    \"\"\"\n",
    "    np.random.seed(seed)\n",
    "    \n",
    "    train_list, valid_list, test_list = [], [], []\n",
    "    \n",
    "    for _, user_df in df.groupby(\"user\"):\n",
    "        user_df = user_df.sample(frac=1, random_state=seed)\n",
    "        \n",
    "        num_interactions = len(user_df)\n",
    "        train_end = int(num_interactions * train_ratio)\n",
    "        valid_end = train_end + int(num_interactions * valid_ratio)\n",
    "        \n",
    "        train_list.append(user_df.iloc[:train_end])\n",
    "        valid_list.append(user_df.iloc[train_end:valid_end])\n",
    "        test_list.append(user_df.iloc[valid_end:])\n",
    "    \n",
    "    train_df = pd.concat(train_list).reset_index(drop=True)\n",
    "    valid_df = pd.concat(valid_list).reset_index(drop=True)\n",
    "    test_df = pd.concat(test_list).reset_index(drop=True)\n",
    "    \n",
    "    # Ensure valid & test sets contain only users & items from training\n",
    "    train_users, train_items = set(train_df['user']), set(train_df['item'])\n",
    "    \n",
    "    valid_df = valid_df[valid_df['user'].isin(train_users) & valid_df['item'].isin(train_items)].reset_index(drop=True)\n",
    "    test_df = test_df[test_df['user'].isin(train_users) & test_df['item'].isin(train_items)].reset_index(drop=True)\n",
    "    \n",
    "    return train_df, valid_df, test_df\n",
    "\n",
    "# split data into train, valid, test\n",
    "train, valid, test = split_data(multimodal_ratings)\n",
    "\n",
    "print(f'Len training: {len(train)}')\n",
    "print(f'Len validation: {len(valid)}')\n",
    "print(f'Len testing: {len(test)}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "# format data into MMRec format\n",
    "# MMRec requires a single .inter file, with a column named 'x_label' that can assume 3 possible values:\n",
    "# - 0: the interaction is in the training set\n",
    "# - 1: the interaction is in the validation set\n",
    "# - 2: the interaction is in the test set\n",
    "train['x_label'] = 0\n",
    "valid['x_label'] = 1\n",
    "test['x_label'] = 2\n",
    "\n",
    "# now, we concat the three dataframe to save the unique .inter file required by MMRec\n",
    "split_data = pd.concat([train, valid, test])\n",
    "\n",
    "# binarize rating\n",
    "#Â if rating <= 3    --> 0\n",
    "#Â if rating > 4     --> 1\n",
    "split_data['rating'] = split_data['rating'].apply(lambda x: 0 if x <= 3 else 1)\n",
    "\n",
    "# rename columns\n",
    "split_data.columns = ['userID', 'itemID', 'rating', 'timestamp', 'x_label']\n",
    "\n",
    "split_data.to_csv('movielens_1m.inter', sep='\\t', index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "# build the maps for users and item, so that their IDs go from 0 to x-1\n",
    "\n",
    "map_user = {userID: new_id for new_id, userID in enumerate(split_data['userID'].unique())}\n",
    "map_item = {itemID: new_id for new_id, itemID in enumerate(split_data['itemID'].unique())}\n",
    "\n",
    "inverse_map_item = {new_id: itemID for itemID, new_id in map_item.items()}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "whisper\n",
      "all-mpnet-base-v2\n",
      "r2p1d\n",
      "vit_cls\n",
      "all-MiniLM-L6-v2\n",
      "vggish\n",
      "resnet152\n",
      "vit_avg\n",
      "i3d\n",
      "vgg\n"
     ]
    }
   ],
   "source": [
    "# for each dict of embedding:\n",
    "# 1. get the items that must be kept\n",
    "# 2. map with the new IDs\n",
    "# 3. store them as np.array\n",
    "\n",
    "for pkl_file in os.listdir('multimodal_features/dict/'):\n",
    "\n",
    "    if '.pkl' in pkl_file:\n",
    "\n",
    "        name_mod = pkl_file.split('.')[0]\n",
    "        print(name_mod)\n",
    "\n",
    "        emb_list = []\n",
    "        \n",
    "        data_dict = pkl.load(open(f'multimodal_features/dict/{pkl_file}', 'rb'))\n",
    "\n",
    "        for index in range(len(inverse_map_item)):\n",
    "            old_id = inverse_map_item[index]\n",
    "            emb = data_dict[old_id]\n",
    "            emb_list.append(emb)\n",
    "        \n",
    "        emb_array = np.array(emb_list)\n",
    "        np.save(open(f'multimodal_features/mmrec_npy/{name_mod}.npy', 'wb'), emb_array)\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Now we remap the original dataset and save the remapped version\n",
    "split_data['userID'] = split_data['userID'].map(map_user)\n",
    "split_data['itemID'] = split_data['itemID'].map(map_item)\n",
    "\n",
    "split_data.to_csv('processed_data/movielens_1m.inter', sep='\\t', index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "whisper.pkl\n",
      "all-mpnet-base-v2.pkl\n",
      "r2p1d.pkl\n",
      "vit_cls.pkl\n",
      "all-MiniLM-L6-v2.pkl\n",
      "vggish.pkl\n",
      "resnet152.pkl\n",
      "vit_avg.pkl\n",
      "i3d.pkl\n",
      "vgg.pkl\n"
     ]
    }
   ],
   "source": [
    "# just to be sure that numpy library version mismatch should affect the readability of our embeddings, \n",
    "# we save the multimodal features as json files as well\n",
    "\n",
    "import pickle as pkl\n",
    "import os \n",
    "import numpy as np\n",
    "import json\n",
    "\n",
    "item_keys = None\n",
    "\n",
    "for pkl_file in os.listdir('multimodal_features/dict/'):\n",
    "\n",
    "    if '.pkl' in pkl_file:\n",
    "        print(pkl_file)\n",
    "        data_dict = pkl.load(open(f'multimodal_features/dict/{pkl_file}', 'rb'))\n",
    "        \n",
    "        data_dict_serializable = {key: value.tolist() for key, value in data_dict.items()}\n",
    "        with open(f'multimodal_features/json/{pkl_file.split(\".pkl\")[0]}.json', 'w') as f:\n",
    "            json.dump(data_dict_serializable, f, indent=4)\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "_env",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
